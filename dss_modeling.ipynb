{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d853da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "import string as st\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from nltk import PorterStemmer, WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ce0bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"/Users/omar.hassan/Documents/DAEN/DAEN 690/dss_cleanv2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbc2445",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.convert_dtypes()\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3a6ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna(subset = ['comments'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562f43dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data['complaint'].fillna('Yes', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0009f4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how the labels are distributed\n",
    "print(np.unique(data['complaint']))\n",
    "print(np.unique(data['complaint'].value_counts()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1f35f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all punctuations from the text\n",
    "\n",
    "def remove_punct(text):\n",
    "    return (\"\".join([ch for ch in text if ch not in st.punctuation]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9ece82",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['removed_punc'] = data['comments'].apply(lambda x: remove_punct(x))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2ca242",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Convert text to lower case tokens. Here, split() is applied on white-spaces. But, it could be applied\n",
    "    on special characters, tabs or any other string based on which text is to be seperated into tokens.\n",
    "'''\n",
    "def tokenize(text):\n",
    "    text = re.split('\\s+' ,text)\n",
    "    return [x.lower() for x in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b762b366",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tokens'] = data['removed_punc'].apply(lambda msg : tokenize(msg))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b245d846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove tokens of length less than 3\n",
    "def remove_small_words(text):\n",
    "    return [x for x in text if len(x) > 3 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b8be76",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['filtered_tokens'] = data['tokens'].apply(lambda x : remove_small_words(x))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884d4f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f81939b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3a44d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Remove stopwords. Here, NLTK corpus list is used for a match. However, a customized user-defined \n",
    "    list could be created and used to limit the matches in input text. \n",
    "'''\n",
    "def remove_stopwords(text):\n",
    "    return [word for word in text if word not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878c3583",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['clean_tokens'] = data['filtered_tokens'].apply(lambda x : remove_stopwords(x))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d099234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply lemmatization on tokens\n",
    "def lemmatize(text):\n",
    "    word_net = WordNetLemmatizer()\n",
    "    return [word_net.lemmatize(word) for word in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a7453c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['lemma_words'] = data['clean_tokens'].apply(lambda x : lemmatize(x))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220e1d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sentences to get clean text as input for vectors\n",
    "\n",
    "def return_sentences(tokens):\n",
    "    return \" \".join([word for word in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f789c397",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['clean_text'] = data['lemma_words'].apply(lambda x : return_sentences(x))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d175cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a basic word cloud \n",
    "from wordcloud import WordCloud, ImageColorGenerator\n",
    "\n",
    "text = \" \".join([x for x in data['clean_text']])\n",
    "# Create and generate a word cloud image:\n",
    "wordcloud = WordCloud(max_font_size=30, max_words=1000).generate(text)\n",
    "\n",
    "# Display the generated image:\n",
    "plt.figure(figsize= [20,10])\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59267f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for the model. Convert label in to binary\n",
    "\n",
    "data['complaint'] = [1 if x == 'Yes' else 0 for x in data['complaint']]\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658d08d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(data['clean_text'], data['complaint'], test_size=0.2, random_state = 5)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9c74ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "tfidf_train = tfidf.fit_transform(X_train)\n",
    "tfidf_test = tfidf.transform(X_test)\n",
    "\n",
    "print(tfidf_train.toarray())\n",
    "print(tfidf_train.shape)\n",
    "print(tfidf_test.toarray())\n",
    "print(tfidf_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07320d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(max_iter = 500)\n",
    "lr.fit(tfidf_train, y_train)\n",
    "print('Logistic Regression model fitted..')\n",
    "\n",
    "ypred = lr.predict(tfidf_test)\n",
    "print(\"Accuracy score : {}\".format(accuracy_score(y_test, ypred)))\n",
    "print(\"Confusion matrix : \\n {}\".format(confusion_matrix(y_test, ypred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c63f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_accuracy = accuracy_score(y_test,ypred)\n",
    "print('Accuracy:',lr_accuracy)\n",
    "\n",
    "# passing actual and predicted values\n",
    "lr_cm = confusion_matrix(y_test, ypred)\n",
    "\n",
    "# true Write data values in each cell of the matrix\n",
    "plt.figure(figsize = (15,8))\n",
    "sns.heatmap(lr_cm, annot=True, fmt='.0f')\n",
    "plt.savefig('confusion.png')\n",
    "\n",
    "lr_cr = classification_report(y_test, ypred)\n",
    "print('Classification Report:')\n",
    "print (lr_cr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cd3196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn_clf=KNeighborsClassifier()\n",
    "knn_clf.fit(tfidf_train,y_train)\n",
    "ypred=knn_clf.predict(tfidf_test) #These are the predicted output values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96918718",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_accuracy = accuracy_score(y_test,ypred)\n",
    "print('Accuracy:',knn_accuracy)\n",
    "\n",
    "# passing actual and predicted values\n",
    "knn_cm = confusion_matrix(y_test, ypred)\n",
    "\n",
    "# true Write data values in each cell of the matrix\n",
    "plt.figure(figsize = (15,8))\n",
    "sns.heatmap(knn_cm, annot=True, fmt='.0f')\n",
    "plt.savefig('confusion.png')\n",
    "\n",
    "knn_cr = classification_report(y_test, ypred)\n",
    "print('Classification Report:')\n",
    "print (knn_cr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccab31ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Gaussian Naive Bayes model\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# create a Gaussian Classifier\n",
    "classifer = GaussianNB()\n",
    "\n",
    "# train the model using the training sets\n",
    "classifer.fit(tfidf_train.toarray(), y_train)\n",
    "\n",
    "# predict the response for test dataset\n",
    "y_pred = classifer.predict(tfidf_test.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9578ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_accuracy = accuracy_score(y_test,y_pred)\n",
    "print('Accuracy:',nb_accuracy)\n",
    "\n",
    "# passing actual and predicted values\n",
    "nb_cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# true Write data values in each cell of the matrix\n",
    "plt.figure(figsize = (15,8))\n",
    "sns.heatmap(nb_cm, annot=True, fmt='.0f')\n",
    "plt.savefig('confusion.png')\n",
    "\n",
    "nb_cr = classification_report(y_test, y_pred)\n",
    "print('Classification Report:')\n",
    "print (nb_cr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
